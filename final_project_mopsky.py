# -*- coding: utf-8 -*-
"""Final Project MOPSKY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16c1ze8mE8nNUOTiajXR8LwWr_jK21H8q

Group : MOPSKY (Group 3)

Team Leader : Laudavian D. Sukardi

Anggota :
- A Ganing Permata
- Ihsan Kamil
- Irwan Syah Bana
- M Fikri Fernanda Yusuf
- Rana Ayunda Salsabila

Mentor: Stephani Fani

# 1. Problem Statement

MOPSKY Team menciptakan HPKuLaku Tech yang merupakan suatu perusahaan startup yang bergerak di bidang finansial dengan menyediakan jasa peminjaman uang yang diperoleh dari penukaran smartphone dengan menggunakan aplikasi yaitu HPKuLaku. Kami membantu investor dan/atau perusahaan finansial lain dengan skema joint venture atau acquisition untuk meningkatkan keuntungan dengan model bisnis yang unik dan berbeda dengan perusahaan fintech lainnya.

HPKuLaku merupakan aplikasi smartphone yang digunakan untuk menentukan kriteria pengklasifikasian rentang harga smartphone untuk mengetahui jumlah pinjaman yang akan diberikan kepada pihak yang meminjamkan smartphone-nya.
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

print('numpy version : ',np.__version__)
print('pandas version : ',pd.__version__)
print('seaborn version : ',sns.__version__)

sns.set(rc={'figure.figsize':(20.7,8.27)})
sns.set_style("whitegrid")
sns.color_palette("dark")
plt.style.use("fivethirtyeight")

"""# 2. Exploratory Data Analysis




"""

testdata = pd.read_csv('/content/test.csv')
traindata = pd.read_csv('/content/train.csv')

# Copy data train awal in case diperlukan untuk pemrosesan data dari awal
traindata_ori = traindata.copy()

traindata.head()



testdata.head() # set yang belum ada grouping pricerange. INI YANG AKAN DIPAKAI BUAT PENGUJIAN MACHINE LEARNING

traindata.info()

traindata.describe()

"""Insight
- px height ada yang 0. kayaknya salah input
- Terdapat data bertipe kategorikal namun sudah dalam bentuk ordinal, yaitu `blue`, `dual_sim`, `four_g`, `three_g`, `touchcreen`, dan `wifi`, selain itu data bertipe integer, kecuali `clock_speed` yaitu float.
- Dari info data, tidak terdapat missing value
- Terdapat 2000 data pada dataframe

## 2.1 Memisahkan data bertipe kategorikal dan numerikal
"""

kategorikal = ['blue','dual_sim','three_g','four_g','touch_screen','wifi']
numerikal = ['battery_power','clock_speed','fc','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','price_range']

"""## 2.2 Statistics and Simple Exploratory Data Analysis"""

traindata.sample(10)

"""Insight:
- sample tidak menunjukkan adanya feature dengan nilai yang tidak sesuai dengan nama kolom. data OK
"""

traindata[numerikal].describe()

"""Insights:
- kolom dengan mean yang tidak jauh dari median: `battery_power`, `clock_speed`, `int_memory`, `m_dep`, `mobile_wt`, `n_cores`, `pc`, `px_width`, `ram`, `sc_h`, `sc_w`, `talk_time`, `price_range`
- data-data yang ada banyak yang memiliki nilai min ataupun max yang jauh dari median. hal ini dapat dimaklumi mengingat data ini merupakan nilai yang cenderung "relatif" terhadap hal2 yang diluar scope data yang disediakan.
- meskipun nilai nilai min dan max pada beberapa data berada jauh dari median, nilai min dan max masih merupakan data yang bisa dibilang wajar sehingga kemungkinan tidak diperlukan drop data.
"""

for col in kategorikal:
    print(f'''Value count kolom {col}:''')
    print(traindata[col].value_counts())
    print()

"""Insights:
- mengingat data yang seharusnya bersifat kategorikal ditulis dengan angka (menjadi data ordinal), maka ada dua opsi yang dapat dilakukan kedepannya yaitu:
  - merubah data menjadi nominal
  - membiarkan data tetap dalam versi numerikal agar perhitungan ratio lebih mudah
- nilai min max seluruh kolom yang hanya menampilkan angka 1 dan 0 secara tidak langsung menunjukkan bahwa hanya angka 0 dan 1 yang digunakan untuk data ini dimana angka 0 dapat diterjemahkan sebagai "tidak / no" dan angka 1 dapat diterjemahkan sebagai "yes / ya"
- dari dua dataset yang tersedia, karakteristik yang kita ketahui adalah: data memiliki target dan tipe data bisa dianggap bersifat kategorikal. maka metode machine learning yang akan digunakan adalah `Supervised Learning - Klasifikasi`.
"""

for col in kategorikal:
  print(f'''value ratio kolom {col}:''')
  print(traindata[col].mean())
  print()

plt.figure(figsize=(18, 4))
for i in range(0, len(numerikal)):
    plt.subplot(1, len(numerikal), i+1)
    sns.boxplot(y=traindata[numerikal[i]], color='gray', orient='v')
    plt.tight_layout()

"""Insight boxplot:
- visualisasi data yang ditunjukkan boxplot memperlihatkan bahwa ada 3 outlier di feature `front kamera`. Kemungkinan hal ini dianggap outlier secara statistik namun bila dilihat max value dari statistik deskriptif diatas dapat disimpulkan bahwa data outlier masih termasuk data yang logis. Dalam hal ini outlier hanya akan dihilangkan apabila mengganggu proses data statistik selanjutnya.
"""

plt.figure(figsize=(14, 10))
sns.heatmap(traindata.corr(), cmap='Blues', annot=True, fmt='.2f');

"""Heatmap Insight:
- 1. Correlation heatmap menunjukkan bahwa feature utama yang paling berpengaruh terhadap target `price_range` adalah besarnya `RAM` yang disematkan pada ponsel, selain itu terdapat feature yang berkorelasi lemah yaitu `battery_power`, `px_height`, dan `px_widht`
- 2. terdapat korelasi antara `pc`(primary camera) dan `fc`(front camera) yang positif. Ada dua kemungkinan yang bisa terjadi disini.
    - bila tidak ada data dengan nilai 0, kualitas `front camera` umumnya memiliki perbandingan tertentu dengan kualitas `primary camera`
    - bila ada data dengan nilai 0 maka semakin tinggi kualitas front/primary camera menunjukkan kemungkinan disematkannya front / primary camera sebagai fitur tambahan di ponsel tersebut
- 3. terdapat korelasi antara `four g` dengan `three g`.
- 4. Untuk memprediksi harga bisa mencoba dengan menggunakan fitur `battery_power`, `ram`, `px_height`, `px_width`, dan `int_memory` 
"""

#melihat distribusi data numerik
plt.figure(figsize=(20, 15))
for i in range(0, len(numerikal)):
    plt.subplot(5, len(numerikal)/2, i+1)
    sns.distplot(traindata[numerikal[i]], color='gray')
    plt.tight_layout()

"""Insight distplot:
- kebanyakan data berdistribusi normal dengan pengecualian `px_height`, `sc_w`, `clock_speed`, dan `fc`. Data-data ini akan menggunakan median untuk perwakilan data.
"""

# Melihat distribusi data kategorikal
plt.figure(figsize=(20, 15))
for i in range(0, len(kategorikal)):
    plt.subplot(5, len(kategorikal)/2, i+1)
    sns.distplot(traindata[kategorikal[i]])
    plt.tight_layout()

"""Insight

- Perbandingan `0` dengan `1` rata-rata tidak terlalu jauh kecuali pada kolom `three_g`
"""

traindata[['battery_power','int_memory','m_dep','mobile_wt','n_cores','pc','px_width','ram','sc_h','talk_time','price_range']].groupby('price_range').mean().reset_index()

"""Insight groupby data:
- `battery power` ada sedikit korelasi yang dapat dilihat juga dari hasil correlation heatmap.
- sesuai dengan hasil heatmap, `ram` memiliki korelasi yang
- ada kemungkinan kita bakalan pake logistic regression lagi kayak di homework mengingat price range bersifat kategorikal
- kalau mau pake regresi linear, pake ram terhadap price range
- adain perbandingan mean antara masing2 price range
- cek distribusi datanya gimana normal / skew / bi?

# 3. Feature Engineering (Optional)

## 3.1 Penambahan Fitur dan Melihat Korelasi Dengan Target
- by: Rana
"""

traindata_ori['cellular'] = np.where((traindata_ori['three_g']==1) & (traindata_ori['four_g']==1),1,0)
traindata_ori['px_ratio'] = traindata_ori['px_height'] * traindata_ori['px_width']
traindata_ori['sc_area'] = traindata_ori['sc_h'] * traindata_ori['sc_w']
traindata_ori['slim_ts_phone'] = np.where((traindata_ori['m_dep']<0.7) & (traindata_ori['touch_screen']==1),1,0)

ft1 = traindata_ori[['price_range', 'cellular', 'px_ratio', 'sc_area', 'slim_ts_phone']].corr()
sns.heatmap(ft1, annot=True);

"""Insight:

- Penambahan feature pada data yang sudah ada tidak, memiliki korelasi dengan target `price_range` namun `px_ratio` memiliki korelasi positif yang rendah

# 3. Data Visualization

## 3.1 Price Range Data Distribution
"""

value_counts = traindata['price_range'].value_counts()
plt.pie(value_counts.values, labels = value_counts.index, autopct='%1.1f%%', startangle=90)
plt.title("Price Range Data Distribution") 
plt.show()

"""## 3.2 Battery Power"""

sns.displot(x='battery_power', data=traindata, kind='kde');

# battery_power dan price_range
plt.figure(figsize=(8,6))
sns.boxplot(y='battery_power', x='price_range', data=traindata);

"""Insight:
- Plot berwarna biru merepresentasikan bahwa, hp dengan harga murah lebih banyak yg memiliki kapasitas `battery_power` sebesar 800-1400 mAH
- Plot berwarna hijau merepresentasikan bahwa, hp dengan harga mahal lebih banyak yg memiliki kapasitas `battery_power` sebesar 1100-1700 mAH

## 3.3 RAM
"""

sns.displot(x='ram', data=traindata, kind='kde');

# ram dan price_range
plt.figure(figsize=(8,4))
sns.boxplot(y = 'ram',x = 'price_range',data = traindata);

"""Insight:
- Semakin kecil `ram` maka semakin murah harga hp, dan semakin besar `ram` maka semakin mahal harga hp

## 3.4 Px Height dan Px Width
"""

# px_widht dan price_range
plt.figure(figsize=(8,4))
sns.boxplot(x='price_range', y='px_width', data=traindata);

# px_height dan price_range
plt.figure(figsize=(8,6))
sns.boxplot(x='price_range', y='px_height', data=traindata);

"""Insight:

Semakin kecil tinggi baik pada `px_height` maupun `px_width` maka semakin besar `price_range`.

## 3.5 Internal Memory
"""

sns.displot(x='int_memory', data=traindata, kind='kde');

plt.figure(figsize=(8, 6))
sns.boxplot(x='price_range', y='int_memory', data=traindata);

"""Insight

- Distribusi int_memory di setiap price_range cenderung merata

# 4. Data Preparation

## 4.1 Data Cleansing

### 4.1.1 Missing Value
Cek apakah ada fitur yg nilainya hilang/NaN?
"""

# Cek apakah ada missing value
traindata.isna().sum()

"""Tidak ada missing value dari data diatas

### 4.1.2 Duplicate
Cek apakah ada fitur yg nilainya sama/duplikat?
"""

# Cek apakah ada duplicate
traindata.duplicated(subset = ['battery_power','blue','clock_speed','dual_sim','fc','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','three_g','touch_screen','wifi','price_range']).sum()

"""Insight:
- Tidak ada duplicate value dari data diatas

### 4.1.3 Outlier
Cek apakah ada outlier?
"""

traindata.describe()

plt.figure(figsize=(18, 4))
for i in range(0, len(numerikal)):
    plt.subplot(1, len(numerikal), i+1)
    sns.boxplot(y=traindata[numerikal[i]], color='gray', orient='v')
    plt.tight_layout()

"""Insight:
- Terlihat bahwa pada fitur `fc` terdapat outlier, tetapi outlier tersebut tidak perlu dihapus karna jika dilihat data dataset diatas, data pada fitur `fc` terbilang normal

## 4.2 Encoding
"""

traindata.info()

"""Insight:
- Tidak ada tipe data yg categorical, jadi tidak perlu diubah menjadi numeric

## 4.3 Class Imbalance
"""

traindata.info()

"""Insight:
- Tidak ada fitur yg bertipe data bool, jadi tidak perlu menggunakan class imbalance

## 4.4 Normalisasi/Standarisasi
- Gunakan standardization bila kita tahu data punya sebaran normal/gaussian
- Gunakan standardization bila model yang kita pakai punya asumsi tentang normalitas (e.g. regresi linear)
- Gunakan normalization apabila tidak memenuhi 2 kriteria di atas
"""

#melihat distribusi
plt.figure(figsize=(20, 15))
for i in range(0, len(numerikal)):
    plt.subplot(5, len(numerikal)/2, i+1)
    sns.distplot(traindata[numerikal[i]], color='gray')
    plt.tight_layout()

"""Insight:
- Berdasarkan plot diatas, rata-rata fitur cenderung berdistribusi normal, jadi tidak perlu di normalisasi. Bisa dicoba untuk standarisasi

### 4.3.1 Standarisasi
Mari kita buat 2 perbandingan antara di Standarisasi dan tidak
"""

nostd_traindata = traindata.copy() # Mengcopy data

#from sklearn.preprocessing import MinMaxScaler, StandardScaler
#for column in ['battery_power','int_memory','ram','px_height','px_width']:
#    traindata[column] = StandardScaler().fit_transform(traindata[column].values.reshape(len(traindata),1))

# Mengecek apakah sudah ter-Standarisasi
traindata.head()

traindata.describe()

# Tanpa standarisasi
nostd_traindata.head()

"""## 4.5 Split Train n Test"""

from sklearn.model_selection import train_test_split

# Dengan data yg di Standarisasi
X_train, X_test, y_train, y_test = train_test_split(traindata.drop('price_range', axis=1), traindata['price_range'], test_size=0.50, random_state=42)

# Tanpa std
r_train, r_test, s_train, s_test = train_test_split(nostd_traindata.drop('price_range', axis=1), 
                                                        nostd_traindata['price_range'],test_size=0.50, random_state=42)

# x = traindata['ram']
# y = traindata['price_range']

# from sklearn.model_selection import train_test_split
# X2_train, X2_test, ytrain, ytest = train_test_split(traindata.drop('ram',axis=1),traindata.drop('price_range',axis=1),test_size=1/2,random_state=101)

"""# 5. Modelling

"""

# #Fit model simple linear regression 
# from sklearn.linear_model import LinearRegression
# regressor = LinearRegression()
# regressor.fit(X_train, y_train)

# pred = regressor.predict(X_test)

# from sklearn.metrics import mean_absolute_error
# from sklearn.metrics import mean_squared_error
# from sklearn.metrics import r2_score
# from sklearn.metrics import classification_report
# print(mean_absolute_error(y_train,y_test))
# print(mean_squared_error(y_train,y_test,squared=False))
# print(r2_score(y_test,pred))
# print(classification_report(y_train,y_test))

"""experimental section.."""

# xtrn = traindata[['ram']]
# ytrn = traindata[['price_range']]
# xtst = testdata[['ram']]

# from sklearn.linear_model import LinearRegression
# regr = LinearRegression()

# regr.fit(xtrn,ytrn)

# ytst = regr.predict(xtst)

# from sklearn.metrics import mean_absolute_error
# from sklearn.metrics import mean_squared_error
# from sklearn.metrics import r2_score
# from sklearn.metrics import classification_report
# print(mean_absolute_error(ytrn,ytst))
# print(mean_squared_error(ytrn,ytst,squared=False))
# print(r2_score(ytst,pred))
# print(classification_report(ytrn,ytst))

"""<!-- ## 5.2 Logistic Regression -->"""

# 5.3 K-nearest Neighbor

# # Predict the test data
# y_predicted = knn.predict(X_test)

# #Evaluation

# from sklearn.metrics import classification_report, confusion_matrix
# print('\nconfustion matrix') # generate the confusion matrix
# print(confusion_matrix(y_test, y_predicted))

# from sklearn.metrics import accuracy_score
# print('\naccuracy')
# print(accuracy_score(y_test, y_predicted))

# from sklearn.metrics import classification_report
# print('\nclassification report')
# print(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num

# ### Tune Hyperparameter with RandomSearch
# from sklearn.model_selection import RandomizedSearchCV
# from scipy.stats import uniform

# #List Hyperparameters yang akan diuji
# n_neighbors = list(range(1,30))
# p=[1,2]

# #Menjadikan ke dalam bentuk dictionary
# hyperparameters = dict(n_neighbors=n_neighbors, p=p)

# # Init Logres dengan Gridsearch, cross validation = 5
# knn2 = KNeighborsClassifier()
# clf = RandomizedSearchCV(knn2, hyperparameters, cv=5, random_state=42)

# #Fitting Model
# best_model = clf.fit(X_train, y_train)

# #Prediksi menggunakan model baru
# y_pred = best_model.predict(X_test)#Check performa dari model
# print(classification_report(y_test, y_pred))
# # roc_auc_score(y_test, y_pred)

"""# 5.1 Experiment Vian

### 5.1.1 Logistice Regression
"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)
from sklearn.metrics import accuracy_score
print("accuracy :", accuracy_score(y_test,y_pred))

from sklearn.metrics import precision_score
print("precision :", precision_score(y_test,y_pred, average=None))

from sklearn.metrics import classification_report
target_names = ['class 0', 'class 1', 'class 2','class 3']
print(classification_report(y_test,y_pred))

from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import roc_auc_score

def multiclass_roc_auc_score(y_test, y_pred, average="macro"):
    lb = LabelBinarizer()
    lb.fit(y_test)
    y_test = lb.transform(y_test)
    y_pred = lb.transform(y_pred)
    return roc_auc_score(y_test, y_pred, average=average)

multiclass_roc_auc_score(y_test, y_pred)

#buat tabel hasil untuk semua accuracy
cols = ['Model','AUC Score','Precision','Recall','f1','Accuracy','Train ACC']
#dataframe kosong untuk diisi oleh hasil accuracy
tabel_hasil = pd.DataFrame(columns = cols)
#input informasi ke tabel
from sklearn import metrics
logregmetrics = pd.Series({'Model':"Logistic regression",
                          'AUC Score': multiclass_roc_auc_score(y_test,y_pred),
                          'Precision': metrics.precision_score(y_test,y_pred,average='weighted'),
                          'Recall': metrics.recall_score(y_test,y_pred,average='weighted'),
                          'f1': metrics.f1_score(y_test,y_pred,average='weighted'),
                          'Accuracy': metrics.accuracy_score(y_test,y_pred),
                          'Train ACC': logreg.score(X_train, y_train)})
#append hasil ke tabel
tabel_hasil = tabel_hasil.append(logregmetrics, ignore_index = True)

#lihat hasil tabel
tabel_hasil

"""### 5.1.2 Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dectree = DecisionTreeClassifier()
dectree.fit(X_train, y_train)
dectreepred = dectree.predict(X_test)

#input informasi ke tabel
logregmetrics = pd.Series({'Model':"Decision Tree",
                          'AUC Score': multiclass_roc_auc_score(y_test,dectreepred),
                          'Precision': metrics.precision_score(y_test,dectreepred,average='weighted'),
                          'Recall': metrics.recall_score(y_test,dectreepred,average='weighted'),
                          'f1': metrics.f1_score(y_test,dectreepred,average='weighted'),
                          'Accuracy': metrics.accuracy_score(y_test,dectreepred),
                          'Train ACC': logreg.score(X_train, y_train)})
#append hasil ke tabel
tabel_hasil = tabel_hasil.append(logregmetrics, ignore_index = True)

#lihat hasil tabel
tabel_hasil

"""### 5.1.3 XGBoost"""

from xgboost import XGBClassifier
xgboost = XGBClassifier()
xgboost.fit(X_train, y_train)
xgboostpred = xgboost.predict(X_test)
logregmetrics = pd.Series({'Model':"XGBoost",
                          'AUC Score': multiclass_roc_auc_score(y_test,xgboostpred),
                          'Precision': metrics.precision_score(y_test,xgboostpred,average='weighted'),
                          'Recall': metrics.recall_score(y_test,xgboostpred,average='weighted'),
                          'f1': metrics.f1_score(y_test,xgboostpred,average='weighted'),
                          'Accuracy': metrics.accuracy_score(y_test,xgboostpred),
                          'Train ACC': logreg.score(X_train, y_train)})
#append hasil ke tabel
tabel_hasil = tabel_hasil.append(logregmetrics, ignore_index = True)

#lihat hasil tabel
tabel_hasil

"""### 5.1.4 Random Forest"""

from sklearn.ensemble import RandomForestClassifier

randforest = RandomForestClassifier()
randforest.fit(X_train, y_train)
randforestpred = randforest.predict(X_test)

logregmetrics = pd.Series({'Model':"Random Forest",
                          'AUC Score': multiclass_roc_auc_score(y_test,randforestpred),
                          'Precision': metrics.precision_score(y_test,randforestpred,average='weighted'),
                          'Recall': metrics.recall_score(y_test,randforestpred,average='weighted'),
                          'f1': metrics.f1_score(y_test,randforestpred,average='weighted'),
                          'Accuracy': metrics.accuracy_score(y_test,randforestpred),
                          'Train ACC': logreg.score(X_train, y_train)})
#append hasil ke tabel
tabel_hasil = tabel_hasil.append(logregmetrics, ignore_index = True)

#lihat hasil tabel
tabel_hasil

"""### 5.1.5 K-Nearest Neighbor (KNN)"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
scale = StandardScaler()

X_train_scale = scale.fit_transform(X_train)
X_test_scale = scale.transform(X_test)

param_grid_knn = {'n_neighbors':np.arange(1,15,2), 'p':[1,2,3]}


# instantiate the KNN classifier
knn_Gridsearch = KNeighborsClassifier()

# use 'GridSearchCV' to obtain the best value of K
knn_Gridsearch_CV = GridSearchCV(knn_Gridsearch, param_grid_knn, cv=5)

# fit the model
knn_Gridsearch_CV.fit(X_train_scale,y_train)


knn_Gridsearch_CV.best_params_

# as K = 13, is a best pick for K, build a KNN model with K = 7
knn = KNeighborsClassifier(n_neighbors=13,p=1)

#Fit the model
knn.fit(X_train_scale,y_train)

knn_pred = knn.predict(X_test_scale)


logregmetrics = pd.Series({'Model':"KNN",
                          'AUC Score': multiclass_roc_auc_score(y_test,knn_pred),
                          'Precision': metrics.precision_score(y_test,knn_pred,average='weighted'),
                          'Recall': metrics.recall_score(y_test,knn_pred,average='weighted'),
                          'f1': metrics.f1_score(y_test,knn_pred,average='weighted'),
                          'Accuracy': metrics.accuracy_score(y_test,knn_pred),
                          'Train ACC': logreg.score(X_train, y_train)})
#append hasil ke tabel
tabel_hasil = tabel_hasil.append(logregmetrics, ignore_index = True)

#lihat hasil tabel
tabel_hasil

X_train

nostd_testdata = testdata.copy() # Mengcopy data

from sklearn.preprocessing import MinMaxScaler, StandardScaler
for column in ['battery_power','int_memory','ram','px_height','px_width']:
    testdata[column] = StandardScaler().fit_transform(testdata[column].values.reshape(len(testdata),1))

testingdata = testdata.drop('id', axis=1)
testingdata

xgboosttest = xgboost.predict(testingdata)

testingdata['testpred'] = xgboosttest.tolist()
testingdata

testingdata.describe()

traindata.describe()

"""Insight:

- Accuration tertinggi menggunakan Model XGBoost dengan 89%
- Precision tertinggi menggunakan Model XGBoost yaitu 89%
"""

testingdata.groupby('testpred').mean()

"""#insight data grouping:
group 0 > lowest. battery life luar biasa jelek, ram juga sangat rendah *group yang gak bisa diterima karena gak akan menghasilkan keuntungan*
group 1 > battery dan ram dibawah rata-rata namun masih bisa diterima
group 2 > battery dan ram diatas rata-rata, *low risk loan*
group 3 > flagship model, battery dan ram tertinggi *almost no risk loan*

# 5.2 Experiment Ganing

# 5.3 Experiment Ihsan
"""

# drop unnecessary features
x = traindata.drop('price_range',axis = 1) # features
y = traindata['price_range'] # target

# train and split data
from sklearn.model_selection import train_test_split 
x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.3,random_state = 42)

"""### 5.3.1 Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

traindata = DecisionTreeClassifier(random_state=42)
traindata.fit(x_train,y_train)

# decision tree predict model
y_predicted1 = traindata.predict(x_test)
y_predicted1

# evaluate decision tree model
from sklearn.metrics import classification_report, confusion_matrix
print('\nconfustion matrix') # confusion matrix
print(confusion_matrix(y_test, y_predicted1))

from sklearn.metrics import accuracy_score
print('\naccuracy')
print(accuracy_score(y_test, y_predicted1))

from sklearn.metrics import classification_report
print('\nclassification report')
print(classification_report(y_test, y_predicted1)) # precision, recall, f-1 score, num

"""### 5.3.2 K-Nearest Neighbor (KKN)"""

# standarization
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
x_train_ss = ss.fit_transform(x_train)
x_test_ss = ss.transform(x_test)

# knn
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(x_train_ss, y_train)

y_predicted2 = knn.predict(x_test_ss)
y_predicted2

# evaluate knn model
from sklearn.metrics import classification_report, confusion_matrix
print('\nconfustion matrix') # confusion matrix
print(confusion_matrix(y_test, y_predicted2))

from sklearn.metrics import accuracy_score
print('\naccuracy')
print(accuracy_score(y_test, y_predicted2))

from sklearn.metrics import classification_report
print('\nclassification report')
print(classification_report(y_test, y_predicted2)) # precision, recall, f-1 score, num

"""### 5.3.3 Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(random_state = 42)
logreg.fit(x_train_ss, y_train)

# logreg predict model
y_predicted3 = logreg.predict(x_test_ss)
y_predicted3

# evaluate logreg model
from sklearn.metrics import classification_report, confusion_matrix
print('\nconfustion matrix') # confusion matrix
print(confusion_matrix(y_test, y_predicted3))

from sklearn.metrics import accuracy_score
print('\naccuracy')
print(accuracy_score(y_test, y_predicted3))

from sklearn.metrics import classification_report
print('\nclassification report')
print(classification_report(y_test, y_predicted3)) # precision, recall, f-1 score, num

"""Insight

- Accuration dan Precision tertinggi menggunakan Logistic Regression dengan data yang terstandarisasi yaitu 96%

## 5.4 Experiment Irwan

### 5.4.1 K-Nearest Neighbor (KKN)
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Predict the test data
y_predicted = knn.predict(X_test)

#Evaluation

from sklearn.metrics import classification_report, confusion_matrix
print('\nconfustion matrix') # generate the confusion matrix
print(confusion_matrix(y_test, y_predicted))

from sklearn.metrics import accuracy_score
print('\naccuracy')
print(accuracy_score(y_test, y_predicted))

from sklearn.metrics import classification_report
print('\nclassification report')
print(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num

### Tune Hyperparameter with RandomSearch
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

#List Hyperparameters yang akan diuji
n_neighbors = list(range(1,30))
p=[1,2]

#Menjadikan ke dalam bentuk dictionary
hyperparameters = dict(n_neighbors=n_neighbors, p=p)

# Init Logres dengan Gridsearch, cross validation = 5
knn2 = KNeighborsClassifier()
clf = RandomizedSearchCV(knn2, hyperparameters, cv=5, random_state=42)

#Fitting Model
best_model = clf.fit(X_train, y_train)

#Prediksi menggunakan model baru
y_pred = best_model.predict(X_test)#Check performa dari model
print(classification_report(y_test, y_pred))
# roc_auc_score(y_test, y_pred)

"""5.4.2 Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train,y_train)
y_predicted = dt.predict(X_test)

y_predicted = dt.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
print('\nconfustion matrix') # generate the confusion matrix
print(confusion_matrix(y_test, y_predicted))

from sklearn.metrics import accuracy_score
print('\naccuracy')
print(accuracy_score(y_test, y_predicted))

from sklearn.metrics import classification_report
print('\nclassification report')
print(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num

"""### 5.4.3 Logistic Regression"""

#Logistic regression 
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(random_state=42)
logreg.fit(X_train, y_train) ## pakai yang distandarisasi

## Membuat Prediksi

y_predicted = logreg.predict(X_test)
y_predicted

## Prediksi dalam bentuk probabilitas

y_predicted_proba = logreg.predict_proba(X_test)
y_predicted_proba

## Evaluasi Menggunakan Logistic Regression

from sklearn.metrics import classification_report, confusion_matrix
print('\nconfustion matrix')
print(confusion_matrix(y_test, y_predicted))

from sklearn.metrics import accuracy_score
print('\naccuracy')
print(accuracy_score(y_test, y_predicted))

from sklearn.metrics import classification_report
print('\nclassification report')
print(classification_report(y_test, y_predicted))

"""Insight:

- Accauration dan precision tertinggi menggunakan Model Logistic Regression yaitu 84%

# 5.5 Experiment Fikri

### 5.5.1 Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn import metrics 
import math

"""#### 5.5.1.1 With Standarization"""

loreg = LogisticRegression()
loreg.fit(X_train, y_train)

loreg.score(X_test,y_test)

from sklearn.metrics import classification_report, confusion_matrix

predloreg = loreg.predict(r_test)

print(classification_report(y_test, predloreg))

"""#### 5.5.1.2 Without Standarization"""

nostd_loreg = LogisticRegression()
nostd_loreg.fit(r_train, s_train)

nostd_loreg.score(r_test,s_test)

from sklearn.metrics import classification_report, confusion_matrix

prednostd_loreg = nostd_loreg.predict(r_test)

print(classification_report(y_test, prednostd_loreg))

"""### 5.5.2 K-Nearest Neighbor (KKN)"""

# KNN
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

"""#### 5.5.2.1 With Standarization"""

knn = KNeighborsClassifier(n_neighbors = 7)
knn.fit(X_train, y_train)

knn.score(X_test, y_test) # harusnya lebih tinggi, cek lagi

from sklearn.metrics import classification_report, confusion_matrix

predknn = knn.predict(X_test)

print(classification_report(y_test, predknn))

"""#### 5.5.2.2 Without Standarization"""

nostd_knn = KNeighborsClassifier(n_neighbors = 7)
nostd_knn.fit(r_train, s_train)

nostd_knn.score(r_test, s_test)

from sklearn.metrics import classification_report, confusion_matrix

prednostd_knn = nostd_knn.predict(r_test)

print(classification_report(s_test, prednostd_knn))

"""### 5.5.3 Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

"""#### 5.5.3.1 With Standarization"""

dt = DecisionTreeClassifier()
dt = dt.fit(X_train, y_train)

dt.score(X_test, y_test)

preddt = dt.predict(X_test)

print("Accuracy", metrics.accuracy_score(y_test, preddt))

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, preddt))

"""#### 5.5.3.2 Without Standarization"""

nostd_dt = DecisionTreeClassifier()
nostd_dt.fit(r_train,s_train)

nostd_dt.score(r_test, s_test)

prednostd_dt = nostd_dt.predict(r_test)

print("Accuracy", metrics.accuracy_score(s_test, prednostd_dt))

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(s_test, prednostd_dt))

"""### 5.5.4 Random Forest"""

from sklearn.ensemble import RandomForestClassifier

"""#### 5.5.4.1 With Standarization"""

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(X_train, y_train)

rf.score(X_test, y_test)

predrf = rf.predict(X_test)

print("Accuracy:", metrics.accuracy_score(y_test, predrf))

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, predrf))

"""#### 5.5.4.2 Without Standarization"""

nostd_rf = RandomForestClassifier(n_estimators= 100)
nostd_rf.fit(r_train, s_train)

nostd_rf.score(r_test, s_test)

prednostd_rf = nostd_rf.predict(r_test)

print("Accuracy:", metrics.accuracy_score(s_test, prednostd_rf))

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(s_test, prednostd_rf))

"""Insight:

- Accauration dan precision tertinggi menggunakan Model Random Forest dengan standarisasi yaitu 87%

# 5.6 Experiment Rana

- Coba menggunakan data yang tidak terstandarisasi dan membandingkan seluruh data untuk melihat hasil secara keseluruhan
"""

feature = ['ram', 'battery_power', 'px_ratio']

x = traindata_ori.drop(['price_range'], axis = 1)
y = traindata_ori['price_range']

xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=1/3, random_state=42)

"""## 5.6.1 Logistic Regression"""

from sklearn.linear_model import LogisticRegression

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(random_state=42)
logreg.fit(xtrain, ytrain)

ypredicted = logreg.predict(xtest)
ypredicted

# Dalam bentuk probabilitas

ypredicted_proba = logreg.predict_proba(xtest)
ypredicted_proba

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print('\naccuracy',accuracy_score(ytest, ypredicted))
print('\nclassification report')
print(classification_report(ytest, ypredicted))

"""Insight:

- Accuracy : 55%
- Precision : 55%

## 5.6.2 K-Nearest Neighbor (KKN)
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(xtrain, ytrain)

ypredicted2 = knn.predict(xtest)
ypredicted2

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print('\nconfustion matrix')
print(confusion_matrix(ytest, ypredicted2))
print('\naccuracy',accuracy_score(ytest, ypredicted2))

print('\nclassification report')
print(classification_report(ytest, ypredicted2))

"""Insight:

- Accuracy : 33%
- Precision : 33%

## 5.6.3 Decision Tree
"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(xtrain,ytrain)
ypredicted3 = dt.predict(xtest)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print('\nconfustion matrix') # generate the confusion matrix
print(confusion_matrix(ytest, ypredicted3))
print('\naccuracy',accuracy_score(ytest, ypredicted3))
print('\nclassification report')
print(classification_report(ytest, ypredicted3))

"""Insight:

- Accuracy : 82%
- Precision : 82%

## 5.6.4 Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 100)
rf.fit(xtrain, ytrain)

ypredicted4 = rf.predict(xtest)
ypredicted4

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print('\nconfustion matrix')
print(confusion_matrix(ytest, ypredicted4))
print('\naccuracy',accuracy_score(ytest, ypredicted4))

print('\nclassification report')
print(classification_report(ytest, ypredicted4))

"""Insight:

- Accuracy : 88%
- Precision : 88%

### 5.6.5 Hyperparameter Tuning

Karena Accuracy dan Precision tertinggi menggunakan Metode `Random Forest`, maka akan dilakukan hyperparameter tuning untuk memaksimalkan hasil permodelan dengan menggunakan `Random Forest`
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
import numpy as np

# list of hyperparameter
criterion = ['gini']
min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node
min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node
max_features = ['auto', 'sqrt', 'log2'] # Number of features to consider at every split
min_impurity_split = [0.1, 0.2, 0.3]

hyperparameters = {
    'criterion' : criterion,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
    'max_features': max_features,
    'min_impurity_split' : min_impurity_split
                }
                
dt2 = DecisionTreeClassifier(random_state=42)
clf = RandomizedSearchCV(dt2, hyperparameters, cv=5, random_state=42)

#Fitting Model
best_model = clf.fit(xtrain, ytrain)

#Prediksi menggunakan model baru
y_pred = best_model.predict(xtest)#Check performa dari model
print(classification_report(ytest, ypredicted4))

"""Insight:

- Tidak terjadi perubahan hasil yang signifikan pada model yang dimungkinkan karena jumlah data yang dianalisis sedikit dan hasil sebelumnya (tanpa hyperparameter tuning) sudah bisa dikatakan bagus.

## 5.7 Kesimpulan hasil experiment:

Top 3 permodelan dengan hasil berdasarkan hasil Akurasi:

-  Menggunakan model `Logistic Regression` dengan data yang terstandarisasi yaitu 96%
- Menggunakan model Model XGBoost dengan 89%
- Menggunakan model `Random Forest` dengan tanpa standarisasi yaitu 88%
"""